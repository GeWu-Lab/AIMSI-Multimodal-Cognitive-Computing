<!DOCTYPE html>
<html>
  <head>
    <title>AI Magazine Special Issue</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="./css/main.css">
  </head>

<body>
<div class="page-header">
  <br>
  <table border="0" align="center">
    <tbody>
      <tr>
        <td width="1000" align="center" valign="middle">
          <h1>AI Magazine Special Issue:</h1>
          <h3>Multimodal Cognitive Computing</h3>
        </td>
      </tr>
      <tr>
        <td colspan="6" align="center">
          <h4>
            <a href="https://mc.manuscriptcentral.com/aaai" target="_blank" style="color: yellow;"> Click here to submit full papers. </a>
          </h4>
        </td>
      </tr>
      <tr>
        <td colspan="4" align="center"> 
          <h5>Manuscript submission deadline:&nbsp;<b>September 30, 2022</b></h5>
        </td>
      </tr>
      <br>
    </tbody>
  </table>
  <br>
</div>

<br>
<div class="containertext" style="width: 55%;">
  <!-- <h2 style="text-align:center; color:#159957; font-weight: normal">Overview</h2> -->
  <h2 style="text-align:center; color:#159957; font-weight:bold">Overview</h2>
    <p>Multimodal Cognitive Computing is an emerging research topic in artificial intelligence as the explosion of big multimodal data. 
      <b>It targets to break the traditional boundary between neuroscience and computer science in the field of multi-modality, and paves the way for machines that will have multi-sensory abilities analogous to a human brain.</b>
      For example, sight and hearing are two of the most important senses for human perception, which are slightly discrepant, but the percept is unified with multi-sensory integration. What's more, when there are multiple input senses, human reactions usually perform like a kind of integrative perception by jointly processing multiple modalities, which contributes to accomplishing tasks more accurately and efficiently. By contrast, in the past decades, we mostly concentrated on unimodal perception and have achieved considerable progress in different modalities, such as visual object detection, acoustic speech recognition, natural language dialog, tactile object grasping etc., while paying little attention to the field of multi-modality. Hence, to pursue the human case, the computational multi-modality ability is highly expected for machines, named multimodal cognitive computing. 
    </p>
    <p>One vivid case of the multi-modality ability of humans could be like, "It could be heard that the night view is fragrant and warm, while observed that the string sound is bitter and cold." cited by a poem. In reality, "night", "fragrance", and "warmness" cannot be directly listened, while "string sound", "bitterness", and "coldness" cannot be actually seen. However, the commutative fusion of sensing brings new perception and expression. This typical example of multi-sensory abilities in human beings is named synaesthesia, which is similar to joint sensing such as one seeing colors when listening to music, or having tastes when listening to/speaking vocabulary. As for images, videos, audios, and texts, traditional data processing mainly focuses on the level of the signal, feature, and semantics, namely, perception and feeling, but overlooks synesthesia and the connection among them. Considering such obvious discrepancy in multi-modality processing, 
    <b>this special issue intends to explore the cognitive mechanism of multimodal fusion and focus on multimodal cognitive computing including key scientific issues concerning perceptual reasoning, semantic association, and collaborative learning and its applications.
    </b>
    </p>
    <p>
    Achieving effective multi-modality ability is challenging for computational models, as it is an interdisciplinary research and application field, and uses methods from psychology, biology, signal processing, physics, information theory, mathematics, and statistics. The development of multimodal cognitive computing will cross-fertilize these other research areas with which it interacts. Recently, the rapid development of multimodal machine learning techniques and large-scale low-cost multimodal data have offered promising opportunities. There are many open problems to be defined and to be addressed. This special issue tackles these problems in both academia and industry, and focuses on new foundations and technologies that are intrinsic to multimodal cognitive computing.
    </p>
</div>

<div class="containertext" style="width: 55%">
    <h2 style="text-align:center; color:#159957; font-weight:bold">Scope</h2>
    <p>
      This special issue aims at promoting cutting-edge research along the research direction of multimodal cognitive computing,  from the neuroscience study of humans to the computation models of machine, and offers a timely collection of works to benefit researchers and practitioners, including but not limits to the acquisition of different modalities (e.g., vision, olfaction, tactility, etc.), the modeling and interaction of multimodal data, as well as the potential problems about interpretability, fairness and trustworthiness. This special issue serves as a forum for researchers all over the world to discuss their works and recent advances in this field. 
      <b>We welcome high-quality original submissions addressing both novel theoretical and practical aspects related to multimodal cognitive computing, especially presenting technical reviews and discussions about the existing and potential research topics.
      </b> All submitted papers will be peer-reviewed and selected based on both their quality and their relevance to the theme of this special issue.
    </p>
</div>
    

<div class="containertext" style="width: 55%">
  <h2 style="text-align:left; color:#159957; font-weight:bold">Topics of interests include (but are not limited)</h2>
  <!-- <h2>Topics of interests include, but are not limited to:</h2> -->
    <p><ul>
      <li>Hardware design and modality acquisition</li>
      <li>Synesthesia computing</li>
      <li>Signal encoding and modality modeling</li>
      <li>Multimodal representation learning</li>
      <li>Multimodal interaction and transfer learning</li>
      <li>Physiognomy/Geomancy computing</li>
      <li>Augmented multimodal perception</li>
      <li>Multimodal reinforcement learning</li>
      <li>Multimodal self-supervised learning</li>
      <li>Embodied multimodal learning</li>
      <li>Weakly supervised multimodal learning</li>
      <li>Interpretable multimodal learning</li>
      <li>Multimodal learning with fairness and trustworthiness</li>
    </ul></p>
</div>


<div class="containertext" style="width: 55%">
  <h2 style="text-align:left; color:#159957; font-weight:bold">Paper submission and review</h2>
  <!-- <h2>Paper submission and review</h2> -->
    <p>
      <ul>
      <li>
        Authors need to submit full papers online through the AI Magazine site at 
        <a href="https://mc.manuscriptcentral.com/aaai" target="_blank">Manuscriptcentral</a>.
      </li>
      <li>
        Full length manuscripts are expected to follow the AI Magazine guidelines in 
        <a href="https://onlinelibrary.wiley.com/page/journal/23719621/homepage/author-guidelines" target="_blank">Author-guidelines</a>.
      </li>
      </ul>
    </p>
</div>

<div class="containertext" style="width: 55%">
  <h2 style="text-align:left; color:#159957; font-weight:bold">Important Dates</h2>
  <!-- <h2>Important Dates</h2> -->
    <p>
      <ul>
        <li>Manuscript submission:    September 30, 2022</li>
        <li>First round of reviews:   November 30, 2022</li>
        <li>Second round of reviews:  January 31, 2023</li>
        <li>Final manuscript due:   February 28, 2023</li>
        <li>Expected publication:   March 31, 2023</li>
      </ul>
    </p>
</div>


<div class="containertext" style="width: 55%">
  <h2 style="text-align:left; color:#159957; font-weight:bold">Guest Editors</h2>

    <div class="row">
      <table class="containertext">
        <tr>
          <td style="width:1%"></td>
          <td style="width:25%">
            <a href="https://scholar.google.com.hk/citations?user=ahUibskAAAAJ&hl=zh-CN">
            <div class="imgDiv"><img class="profileDescriptionImg" src="http://iopen.nwpu.edu.cn/__local/C/A3/CA/8BF18EE51A65A3E6BB77535A0D9_F6436B80_3138E.jpg"/></div>
            </a>
          </td>
          <td >
            <a href="https://scholar.google.com.hk/citations?user=ahUibskAAAAJ&hl=zh-CN">
            <b>Prof. Dr. Xuelong Li</b><br>
            </a>
            Professor, Northwestern Polytechnical University, China<br>
            Email: li@nwpu.edu.cn
          </td>
        </tr>
      </table>

      <table class="containertext">
        <tr>
          <td style="width:1%"></td>
          <td style="width:25%">
            <a href="https://dtaoo.github.io/index.html">
            <div class="imgDiv"><img class="profileDescriptionImg" src="https://i.postimg.cc/zfBgrjC6/1.jpg"/></div>
            </a>
          </td>
          <td >
            <a href="https://dtaoo.github.io/index.html">
            <b>Prof. Dr. Di Hu</b><br>
            </a>
            Assistant Professor (tenure-track), Renmin University of China<br>
            Email: dihu@ruc.edu.cn 
          </td>
        </tr>
      </table>

  </div>
</div>

<!-- <br> -->

<div class="page-header">
  <!-- <br> -->
  <p style="height: 0.2em;"></p>
    Welcome to submit manuscripts to our <a href="https://mc.manuscriptcentral.com/aaai" style="color:yellow">Multimodal Cognitive Computing!</a>
  <p style="height: 1em;"></p>
  <!-- <br> -->
</div>


<!--<p align="center" class="acknowledgement">Last updated: 30 July 2012</p>-->

<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

<script>
//openCity(event, 'PST');
document.getElementById("default_button").click(); // Click on the checkbox
function openCity(evt, cityName) {
  // Declare all variables
  var i, tabcontent, tablinks;

  // Get all elements with class="tabcontent" and hide them
  tabcontent = document.getElementsByClassName("tabcontent");
  for (i = 0; i < tabcontent.length; i++) {
    tabcontent[i].style.display = "none";
  }

  // Get all elements with class="tablinks" and remove the class "active"
  tablinks = document.getElementsByClassName("tablinks");
  for (i = 0; i < tablinks.length; i++) {
    tablinks[i].className = tablinks[i].className.replace(" active", "");
  }

  // Show the current tab, and add an "active" class to the button that opened the tab
  document.getElementById(cityName).style.display = "block";
  evt.currentTarget.className += " active";
}

</script>

</body>
</html>
